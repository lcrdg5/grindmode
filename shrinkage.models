# Model reduction exercise 
# creating test and training data
read.csv("C:/users/rone/desktop/stats.4510/CommunityCrime.csv")
community.crime = read.csv("C:/users/rone/desktop/stats.4510/CommunityCrime.csv")
library(glmnet)
set.seed(1)
train.obs = sample(1:nrow(community.crime), .9*nrow(community.crime), replace = FALSE)
cc.train = community.crime[train.obs,]
cc.test = community.crime[-train.obs,]
violent.test = cc.test$ViolentCrimesPerPop

#obtaining test error 

cc.lm = lm(ViolentCrimesPerPop~., data = cc.train)
lm.pred = predict.lm(cc.lm, cc.test)
mean((violent.test-lm.pred)^2)
# test error: . 0547

# fitting a ridge regression model with lambda chosen by cross validation
x = model.matrix(ViolentCrimesPerPop~., data = cc.train)
y = cc.train$ViolentCrimesPerPop
cv.out = cv.glmnet(x,y, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min

ridge.mod = glmnet(x,y, alpha = 0, lambda = bestlam)
dim(coef(ridge.mod))
x.new=model.matrix(ViolentCrimesPerPop~., data = cc.test)
ridge.pred = predict(ridge.mod, s=bestlam, newx = x.new)
mean((ridge.pred - violent.test)^2)
# test error for ridge regression model: .0321

# fitting lasso model with lambda chosen by cross validation

lasso.cc = glmnet(x,y, alpha = 1)
plot(lasso.cc)
set.seed(1)
cv.lasso = cv.glmnet(x, y, alpha = 0)
plot(cv.lasso)
bestlam.lasso = cv.lasso$lambda.min
bestlam
# lambda = .30987
lasso.mod = glmnet(x, y, alpha = 0, lambda = bestlam.lasso)
x.test = model.matrix(ViolentCrimesPerPop~., data = cc.test)
lasso.pred = predict(lasso.mod, newx = x.test, alpha = 1, lambda = bestlam.lasso)
mean((lasso.pred - violent.test)^2)
# test error for lasso model: .03038



# Professor provided code below:
set.seed(1)
n<-80
p<-50
X<-matrix(0,nrow = n, ncol = p)
for (j in 1:p) {
  X[,j]<-runif(n = n, min = 0, max = 1)
}
beta0<-2
betas<-rep(0,p)
betas[1:3]<-c(1,2,3)
betas<-matrix(betas, ncol = 1)
Y<-beta0+X%*%betas+rnorm(n,0,1)

# proffessor provided us the above code  ^^^

# creating a lasso model with a grid from 10^-2 to 1 
lasso.2 = glmnet(X,Y, alpha = 1, lambda = 10^seq(from = -2, to =.1, length = 100))
# trace plot of coeficients vs lambda for all variables
plot(y=coef(lasso.2)[2,], x = lasso.2$lambda, type = "l", ylim = c(-1,3), col = "red")
lines(y=coef(lasso.2)[3,], x = lasso.2$lambda, type = "l", col = "red")
lines(y = coef(lasso.2)[4,], x = lasso.2$lambda, type = "l", col = "red")
for (j in 5:p+1) {
  lines(y = coef(lasso.2)[j,], x = lasso.2$lambda, type = "l", col = "black")
}

# performing the same task above using ridg regression
ridge.2 = glmnet(X,Y, alpha = 0, lambda = 10^seq(from = -2, to = .1, length = 100))
plot(y = coef(ridge.2)[2,], x = ridge.2$lambda, type = "l", ylim = c(-1,3), col = "red")
lines(y = coef(ridge.2)[3,], x = ridge.2$lambda, type = "l", col = "red")
lines(y = coef(ridge.2)[4,], x = ridge.2$lambda, type =  "l", col = "red")
for (j in 5:(p+1)) {
  lines(y = coef(ridge.2)[j,], x = ridge.2$lambda, type = "l", col = 1)
}

